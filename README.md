# Affordability of Housing Prices in Ames, Iowa
## Abstract
The purpose of this analysis is to determine the most important factors that influence affordability (above average, average, below average) of all housing sales in Ames, Iowa that occurred from 2006 to 2009 relative to the rest of the area. We used sale attributes such as lot size, house condition, etc. to see if we can use machine learning to determine why a house is expensive or cheap.

## Contributor
Becky Su

## Reproduction
House Prices dataset using the training set (train.csv), data available on Kaggle compiled by Dean De Cock, accessed in RStudio.
https://www.kaggle.com/datasets/lespin/house-prices-dataset

## Statistical Analysis
In this analysis, we used the machine learning techniques KNN, decision trees, and random forests to determine whether individual houses sold in Ames, Iowa between 2006-2009 were priced above average, average, and below average relative to the rest of the area. The House Prices dataset has 1460 observations with 6 predictors. We converted SalePrice, our response variable, to a new binary column named SaleClass so we can use predictors to classify the response. An above average SalePrice is labelled “expensive” and below average SalePrice is labeled as “cheap”. Diagnostic checking is performed to examine distribution of response variable SalePrice as well as it's relationship with each predictor variable using density function and boxplots. The dataset is then split into training and testing set, where the training set will be used to teach the classification model how to predict. We then apply the algorithm to the test set, and see how accurate the classification was. From decision tree, we found that the model yielded an accuracy rate of 0.855 and an error rate of 0.145 and an AUC of 0.92. K-fold cross validation tree was subsequently performed on the tree to attempt to improve it's accuracy. Pruning the tree reduced the number of relevant variables from 13 to 7 and this method yielded an accuracy rate of 0.856 and an error rate of 0.144. Next we performed K-Nearest Neighbors classification, using K=38 derived from square rooting the total number of obeservations 1460, to find the best number of neighbors 9. Training a 9-NN classifier yielded a relatively low error rate of 0.127. Lastly, we applied RandomForest to our model and yielded our highest accuracy of 0.873, with an AUC of 0.936.

## Conclusion
In comparing all the machine learning methods utilized, we found that randomForest yielded the highest accuracy rate and AUC relative to our model. Since randomForest is less likely to result in overfitting and is more robust against the correlation of predictors than KNN, we conclude that this is the best classification method for our model. 

There isn’t a huge difference in accuracy rates between any of the methods, so each method may have it’s own uses. For example, we have already stated that decision trees are prone to overfitting, but it is also the most easily interpreted method. We were able to prune the tree through K-fold cross validation which allowed us to narrow the model down to the most important variables. We could have also used the variables from the pruned tree to build a simpler randomForest model using only the “important” variables, since a simpler model gives reduces bias and complexity. It would be interesting see if making the randomForest model less complex would be worthwhile by comparing metrics. If we could apply this subset of variables to the randomForest algorithm and come out with a strong model that only utilizes a few independent variables in order to classify at a high success rate, it would lend strength to the argument that OverallQual, LotArea, YearBuilt are the most relevant predictors when it comes to determining whether a house is expensive or not.

As far as further questions we still have, we would want to know how best to balance having a high level of accuracy while also balancing variance and bias so we don’t over/underfit. For example, is it necessary to drop variables from randomForest when the method already inherently accounts for overfitting? However if we just compare for models with the same number of variables, we conclude randomForest is the best for binary classification and our most important predictors for SalePrice are OverallQual, YearBuilt, and LotArea.
